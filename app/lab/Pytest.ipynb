{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is PyTest?\n",
    "\n",
    "Pytest is a testing framework which allows us to write test codes using python. You can write code to test anything like database , API, even UI if you want. But pytest is mainly being used in industry to write tests for APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use PyTest?\n",
    "\n",
    "Some of the advantages of pytest are\n",
    "\n",
    "- Very easy to start with because of its simple and easy syntax.\n",
    "- Can run tests in parallel.\n",
    "- Can run a specific test or a subset of tests\n",
    "- tomatically detect tests\n",
    "- Skip tests\n",
    "- Open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to install PyTest\n",
    "\n",
    "You can installing it by using below command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the installation is completed you can comfirm by running below command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "py.test -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytest framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries.\n",
    "\n",
    "An example of a simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# content of test_sample.py\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def test_answer():\n",
    "    assert inc(3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertions in PyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions are checks that return either True or False status. In pytest, if an assertion fails in a test method, then that method execution is stopped there. The remaining code in that test method is not executed, and pytest will continue with the next test method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"hello\" == \"Hai\" #is an assertion failure.\n",
    "assert 4==4 #is a successful assertion\n",
    "assert True #is a successful assertion\n",
    "assert False #is an assertion failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How pytest identifies the test files and test methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pytest only identifies the file names starting with test_ or ending with _test as the test files. We can explicitly mention other filenames though (explained later). Pytest requires the test method names to start with \"test.\" All other method names will be ignored even if we explicitly ask to run those methods.\n",
    "\n",
    "See some examples of valid and invalid pytest file names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_login.py - valid\n",
    "login_test.py - valid\n",
    "testlogin.py -invalid\n",
    "logintest.py -invalid\n",
    "\n",
    "and methods\n",
    "\n",
    "def test_file1_method1(): - valid\n",
    "def testfile1_method1(): - valid\n",
    "def file1_method1(): - invalid\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest fixtures\n",
    "\n",
    "Fixtures are used when we want to run some code before every test method. So instead of repeating the same code in every test we define fixtures. Usually, fixtures are used to initialize database connections, pass the base , etc\n",
    "\n",
    "A method is marked as a fixture by marking with\n",
    "\n",
    "@pytest.fixture\n",
    "\n",
    "A test method can use a fixture by mentioning the fixture as an input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa=25\n",
    "    bb =35\n",
    "    cc=45\n",
    "    return [aa,bb,cc]\n",
    "\n",
    "def test_comparewithAA(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[0]==zz,\"aa and zz comparison failed\"\n",
    "\n",
    "def test_comparewithBB(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[1]==zz,\"bb and zz comparison failed\"\n",
    "\n",
    "def test_comparewithCC(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[2]==zz,\"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here\n",
    "\n",
    "- We have a fixture named supply_AA_BB_CC. This method will return a list of 3 values.\n",
    "- We have 3 test methods comparing against each of the values.\n",
    "\n",
    "Each of the test function has an input argument whose name is matching with an available fixture. Pytest then invokes the corresponding fixture method and the returned values will be stored in the input argument , here the list [25,35,45]. Now the list items are being used in test methods for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = supply_AA_BB_CC()\n",
    "print(f)\n",
    "test_comparewithAA(f)\n",
    "test_comparewithBB(f)\n",
    "test_comparewithCC(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test test_comparewithBB is passed since zz=BB=35, and the remaining 2 tests are failed.\n",
    "\n",
    "The fixture method has a scope only within that test file it is defined. If we try to access the fixture in some other test file , we will get an error saying fixture 'supply_AA_BB_CC' not found for the test methods in other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized tests\n",
    "\n",
    "The purpose of parameterizing a test is to run a test against multiple sets of arguments. We can do this by @pytest.mark.parametrize.\n",
    "\n",
    "We will see this with the below example. Here we will pass 3 arguments to a test method. This test method will add the first 2 arguments and compare it with the 3rd argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "@pytest.mark.parametrize(\"input1, input2, output\",[(5,5,10),(3,5,12)])\n",
    "def test_add(input1, input2, output):\n",
    "    assert input1+input2 == output,\"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the test method accepts 3 arguments- input1, input2, output. It adds input1 and input2 and compares against the output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "============================================== FAILURES ==============================================\n",
    "__________________________________________ test_add[3-5-12] __________________________________________\n",
    "input1 = 3, input2 = 5, output = 12\n",
    "    @pytest.mark.parametrize(\"input1, input2, output\",[(5,5,10),(3,5,12)])\n",
    "    def test_add(input1, input2, output):\n",
    ">   \tassert input1+input2 == output,\"failed\"\n",
    "E    AssertionError: failed\n",
    "E    assert (3 + 5) == 12\n",
    "test_addition.py:5: AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the tests ran 2 times â€“ one checking 5+5 ==10 and other checking 3+5 ==12\n",
    "\n",
    "test_addition.py::test_add[5-5-10] PASSED\n",
    "\n",
    "test_addition.py::test_add[3-5-12] FAILED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xfail / Skip tests\n",
    "\n",
    "There will be some situations where we don't want to execute a test, or a test case is not relevant for a particular time. In those situations, we have the option to xfail the test or skip the tests\n",
    "\n",
    "The xfailed test will be executed, but it will not be counted as part failed or passed tests. There will be no traceback displayed if that test fails. We can xfail tests using\n",
    "\n",
    "@pytest.mark.xfail.\n",
    "\n",
    "Skipping a test means that the test will not be executed. We can skip tests using\n",
    "\n",
    "@pytest.mark.skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "@pytest.mark.skip\n",
    "def test_add_1():\n",
    "\tassert 100+200 == 400,\"failed\"\n",
    "\n",
    "@pytest.mark.skip\n",
    "def test_add_2():\n",
    "\tassert 100+200 == 300,\"failed\"\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_add_3():\n",
    "\tassert 15+13 == 28,\"failed\"\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_add_4():\n",
    "\tassert 15+13 == 100,\"failed\"\n",
    "\n",
    "def test_add_5():\n",
    "\tassert 3+2 == 5,\"failed\"\n",
    "\n",
    "def test_add_6():\n",
    "\tassert 3+2 == 6,\"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here\n",
    "\n",
    "- test_add_1 and test_add_2 are skipped and will not be executed.\n",
    "- test_add_3 and test_add_4 are xfailed. These tests will be executed and will be part of xfailed(on test failure) or xpassed(on test pass) tests. There won't be any traceback for failures.\n",
    "- test_add_5 and test_add_6 will be executed and test_add_6 will report failure with traceback while the test_add_5 passes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_addition.py::test_add_1 SKIPPED\n",
    "test_addition.py::test_add_2 SKIPPED\n",
    "test_addition.py::test_add_3 XPASS\n",
    "test_addition.py::test_add_4 xfail\n",
    "test_addition.py::test_add_5 PASSED\n",
    "test_addition.py::test_add_6 FAILED\n",
    "\n",
    "============================================== FAILURES ==============================================\n",
    "_____________________________________________ test_add_6 _____________________________________________\n",
    "    def test_add_6():\n",
    ">   \tassert 3+2 == 6,\"failed\"\n",
    "E    AssertionError: failed\n",
    "E    assert (3 + 2) == 6\n",
    "test_addition.py:24: AssertionError\n",
    "\n",
    "================ 1 failed, 1 passed, 2 skipped, 1 xfailed, 1 xpassed in 0.07 seconds ================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results XML\n",
    "\n",
    "We can create test results in XML format which we can feed to Continuous Integration servers for further processing and so. This can be done by\n",
    "\n",
    "py.test test_sample1.py -v --junitxml=\"result.xml\"\n",
    "\n",
    "The result.xml will record the test execution result. Find a sample result.xml below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<testsuite errors=\"0\" failures=\"1\" name=\"pytest\" skips=\"0\" tests=\"2\" time=\"0.046\">\n",
    "   <testcase classname=\"test_sample1\" file=\"test_sample1.py\" line=\"3\" name=\"test_file1_method1\" time=\"0.001384973526\">\n",
    "     <failure message=\"AssertionError:test failed because x=5 y=6 assert 5 ==6\">\n",
    "    @pytest.mark.set1\n",
    "    def test_file1_method1():\n",
    "    \tx=5\n",
    "    \ty=6\n",
    "       \tassert x+1 == y,\"test failed\"\n",
    ">      \tassert x == y,\"test failed because x=\" + str(x) + \" y=\" + str(y)\n",
    "E       AssertionError: test failed because x=5 y=6\n",
    "E       assert 5 == 6\n",
    "         test_sample1.py:9: AssertionError\n",
    "    </failure>\n",
    "   </testcase>\n",
    "   <testcase classname=\"test_sample1\" file=\"test_sample1.py\" line=\"10\" name=\"test_file1_method2\" time=\"0.000830173492432\" />\n",
    "</testsuite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <testsuite errors=\"0\" failures=\"1\" name=\"pytest\" skips=\"0\" tests=\"2\" time=\"0.046\"> we can see a total of two tests of which one is failed. Below that you can see the details regarding each executed test under <testcase> tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pytest framework testing an API\n",
    "\n",
    "Now we will create a small pytest framework to test an API. The API here used is a free one from https://reqres.in/. This website is just to provide testable API. This website doesn't store our data.\n",
    "\n",
    "Here we will write some tests for\n",
    "\n",
    "listing some users\n",
    "login with users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "@pytest.fixture\n",
    "def supply_url():\n",
    "\treturn \"https://reqres.in/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test_list_valid_user tests for valid user fetch and verifies the response\n",
    "- test_list_invaliduser tests for invalid user fetch and verifies the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import requests\n",
    "import json\n",
    "def test_login_valid(supply_url):\n",
    "    url = supply_url + \"/login/\" \n",
    "    data = {'email':'eve.holt@reqres.in','password':'cityslicka'}\n",
    "    resp = requests.post(url, data=data)\n",
    "    j = json.loads(resp.text)\n",
    "    assert resp.status_code == 200, resp.text\n",
    "    assert j['token'] == \"QpwL5tke4Pnpja7X4\", resp.text\n",
    "\n",
    "def test_login_no_password(supply_url):\n",
    "    url = supply_url + \"/login/\" \n",
    "    data = {'email':'test@test.com'}\n",
    "    resp = requests.post(url, data=data)\n",
    "    j = json.loads(resp.text)\n",
    "    assert resp.status_code == 400, resp.text\n",
    "    assert j['error'] == \"Missing password\", resp.text\n",
    "\n",
    "def test_login_no_email(supply_url):\n",
    "    url = supply_url + \"/login/\" \n",
    "    data = {}\n",
    "    resp = requests.post(url, data=data)\n",
    "    j = json.loads(resp.text)\n",
    "    assert resp.status_code == 400, resp.text\n",
    "    assert j['error'] == \"Missing email or username\", resp.text\n",
    "    \n",
    "\n",
    "url = supply_url()\n",
    "test_login_valid(url)\n",
    "test_login_no_password(url)\n",
    "test_login_no_email(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_login_user.py â€“ contains test methods for testing login functionality.\n",
    "\n",
    "- test_login_valid tests the valid login attempt with email and password\n",
    "- test_login_no_password tests the invalid login attempt without passing password\n",
    "- test_login_no_email tests the invalid login attempt without passing email."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_list_user.py::test_list_valid_user[1-George] PASSED                                                                                                                                                                               \n",
    "test_list_user.py::test_list_valid_user[2-Janet] PASSED                                                                                                                                                                                \n",
    "test_list_user.py::test_list_invaliduser PASSED                                                                                                                                                                                        \n",
    "test_login_user.py::test_login_valid PASSED                                                                                                                                                                                            \n",
    "test_login_user.py::test_login_no_password PASSED                                                                                                                                                                                      \n",
    "test_login_user.py::test_login_no_email PASSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Guru99, (July 2019) PyTest Tutorial: What is, Install, Fixture, Assertions. Retrieved from https://www.guru99.com/pytest-tutorial.html#1\n",
    "\n",
    "Holger krekel and pytest-dev team, (2019) pytest: helps you write better programs. Retrieved from https://docs.pytest.org/en/latest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
